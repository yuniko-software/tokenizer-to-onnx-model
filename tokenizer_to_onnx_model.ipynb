{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ONNX Tokenizer Interoperability Example\n",
                "\n",
                "This Jupyter notebook demonstrates converting a tokenizer for an embedding model to ONNX format, using `BAAI/bge-m3` as an example. The ONNX tokenizer, based on XLM-RoBERTa Fast for `bge-m3`, enables **interoperable** text processing in any language supporting ONNX Extensions, such as Python, C#, C++, Java, or JavaScript. In this pipeline, a critical step is converting the ONNX tokenizer outputs (`tokens`, `instance_indices`, `token_indices`) into model-compatible inputs (`input_ids`, `attention_mask`) to work with the transformer model."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Install Required Packages\n",
                "\n",
                "First, let's install all the necessary packages for this notebook."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install onnxruntime onnxruntime-extensions numpy transformers"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Imports\n",
                "\n",
                "Import libraries for ONNX runtime, NumPy, Hugging Face transformers, and file operations. These enable tokenizer conversion and embedding generation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "import onnxruntime as ort\n",
                "import numpy as np\n",
                "from onnxruntime_extensions import gen_processing_models, get_library_path\n",
                "from transformers import AutoTokenizer\n",
                "import os"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Initialize Tokenizer\n",
                "\n",
                "Initialize a Hugging Face tokenizer (using `BAAI/bge-m3` as an example) and convert it to ONNX format. The ONNX tokenizer can be deployed in any language that supports ONNX Extensions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "def initialize_tokenizer(model_type=\"BAAI/bge-m3\", tokenizer_path=\"tokenizer.onnx\"):\n",
                "    \"\"\"Initialize and export the tokenizer if needed.\n",
                "\n",
                "    Args:\n",
                "        model_type (str): Hugging Face model name (e.g., 'BAAI/bge-m3').\n",
                "        tokenizer_path (str): Path to save/load the ONNX tokenizer.\n",
                "\n",
                "    Returns:\n",
                "        tuple: Hugging Face tokenizer and ONNX tokenizer session.\n",
                "    \"\"\"\n",
                "    hf_tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
                "    \n",
                "    if not os.path.exists(tokenizer_path):\n",
                "        print(f\"Generating ONNX tokenizer at {tokenizer_path}\")\n",
                "        tokenizer_model = gen_processing_models(hf_tokenizer, pre_kwargs={}, post_kwargs={})[0]\n",
                "        with open(tokenizer_path, \"wb\") as f:\n",
                "            f.write(tokenizer_model.SerializeToString())\n",
                "    \n",
                "    sess_options = ort.SessionOptions()\n",
                "    sess_options.register_custom_ops_library(get_library_path())\n",
                "    \n",
                "    tokenizer_session = ort.InferenceSession(\n",
                "        tokenizer_path, \n",
                "        sess_options=sess_options, \n",
                "        providers=['CPUExecutionProvider']\n",
                "    )\n",
                "    \n",
                "    return hf_tokenizer, tokenizer_session"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Convert Tokenizer Outputs\n",
                "\n",
                "Convert ONNX tokenizer outputs (`tokens`, `token_indices`) to model inputs (`input_ids`, `attention_mask`). This simplified conversion ensures compatibility with the embedding model. The `attention_mask` is set to all 1s because this pipeline processes a single text without padding or truncation, indicating that all tokens in the sequence are valid and should be fully attended to by the model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "def convert_tokenizer_outputs(tokens, token_indices):\n",
                "    \"\"\"Convert tokenizer outputs to the format expected by the model.\n",
                "\n",
                "    Args:\n",
                "        tokens: Token IDs from ONNX tokenizer.\n",
                "        token_indices: Token position indices.\n",
                "\n",
                "    Returns:\n",
                "        tuple: input_ids and attention_mask as NumPy arrays.\n",
                "    \"\"\"\n",
                "    token_pairs = []\n",
                "    for i in range(len(tokens)):\n",
                "        if i < len(token_indices):\n",
                "            token_pairs.append((token_indices[i], tokens[i]))\n",
                "    \n",
                "    token_pairs.sort()\n",
                "    ordered_tokens = [pair[1] for pair in token_pairs]\n",
                "    \n",
                "    input_ids = np.array([ordered_tokens], dtype=np.int64)\n",
                "    attention_mask = np.ones_like(input_ids, dtype=np.int64)\n",
                "    \n",
                "    return input_ids, attention_mask"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Generate Embedding\n",
                "\n",
                "Generate an embedding for a single text using the ONNX tokenizer and model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_embedding(text, tokenizer_session, model_session):\n",
                "    \"\"\"Generate embedding for a single text.\n",
                "\n",
                "    Args:\n",
                "        text (str): Input text to generate embedding for.\n",
                "        tokenizer_session: ONNX tokenizer session.\n",
                "        model_session: ONNX model session.\n",
                "\n",
                "    Returns:\n",
                "        numpy.ndarray: The sentence embedding.\n",
                "    \"\"\"\n",
                "    tokenizer_outputs = tokenizer_session.run(None, {\"inputs\": np.array([text])})\n",
                "    tokens, _, token_indices = tokenizer_outputs\n",
                "    \n",
                "    input_ids, attention_mask = convert_tokenizer_outputs(\n",
                "        tokens, token_indices\n",
                "    )\n",
                "    \n",
                "    outputs = model_session.run(None, {\n",
                "        \"input_ids\": input_ids,\n",
                "        \"attention_mask\": attention_mask\n",
                "    })\n",
                "    \n",
                "    return outputs[1]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Compare with Hugging Face Tokenizer\n",
                "\n",
                "Compare the embedding generated with the ONNX tokenizer (using `bge-m3` as an example) to the Hugging Face tokenizer's output, confirming that both tokenizers produce equivalent results via cosine similarity."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compare_with_hf_tokenizer(text, hf_tokenizer, tokenizer_session, model_session):\n",
                "    \"\"\"Compare embeddings from ONNX and Hugging Face tokenizers.\n",
                "\n",
                "    Args:\n",
                "        text (str): Input text to tokenize and embed.\n",
                "        hf_tokenizer: Hugging Face tokenizer.\n",
                "        tokenizer_session: ONNX tokenizer session.\n",
                "        model_session: ONNX model session.\n",
                "\n",
                "    Returns:\n",
                "        float: Cosine similarity between embeddings.\n",
                "    \"\"\"\n",
                "    # ONNX tokenizer embedding\n",
                "    onnx_embedding = generate_embedding(text, tokenizer_session, model_session)\n",
                "    \n",
                "    # Hugging Face tokenizer embedding\n",
                "    hf_inputs = hf_tokenizer(text, return_tensors=\"np\")\n",
                "    hf_outputs = model_session.run(None, {\n",
                "        \"input_ids\": hf_inputs[\"input_ids\"],\n",
                "        \"attention_mask\": hf_inputs[\"attention_mask\"]\n",
                "    })\n",
                "    hf_embedding = hf_outputs[1]\n",
                "    \n",
                "    # Calculate cosine similarity\n",
                "    cosine_sim = np.dot(onnx_embedding.flatten(), hf_embedding.flatten()) / (\n",
                "        np.linalg.norm(onnx_embedding) * np.linalg.norm(hf_embedding)\n",
                "    )\n",
                "    \n",
                "    return cosine_sim"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Main Execution\n",
                "\n",
                "Test the ONNX tokenizer pipeline using `bge-m3` as an example. Generate an embedding for a sample text and compare it with the Hugging Face tokenizer's output. To run this pipeline, download `model.onnx` and `model.onnx_data` from https://huggingface.co/BAAI/bge-m3/tree/main/onnx and place them in an `onnx` folder. Note that the `onnx` folder is included in `.gitignore` by default."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Generated embedding shape: (1, 1024)\n",
                        "Sample values: [-0.00892851  0.02104793 -0.01595523 -0.03338689  0.00300002]\n",
                        "Embedding cosine similarity: 1.0\n"
                    ]
                }
            ],
            "source": [
                "# Initialize tokenizer and model\n",
                "hf_tokenizer, tokenizer_session = initialize_tokenizer(tokenizer_path=\"onnx/tokenizer.onnx\")\n",
                "model_session = ort.InferenceSession(\"onnx/model.onnx\", providers=['CPUExecutionProvider'])\n",
                "\n",
                "# Test with a sample text\n",
                "sample_text = \"A test text! Texto de prueba! Текст для теста! 測試文字! Testtext! Testez le texte! Сынақ мәтіні! Тестни текст! परीक्षण पाठ! Kiểm tra văn bản!\"\n",
                "embedding = generate_embedding(sample_text, tokenizer_session, model_session)\n",
                "\n",
                "print(f\"Generated embedding shape: {embedding.shape}\")\n",
                "print(f\"Sample values: {embedding.flatten()[:5]}\")\n",
                "\n",
                "# Compare with Hugging Face tokenizer\n",
                "similarity = compare_with_hf_tokenizer(sample_text, hf_tokenizer, tokenizer_session, model_session)\n",
                "print(f\"Embedding cosine similarity: {similarity}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}